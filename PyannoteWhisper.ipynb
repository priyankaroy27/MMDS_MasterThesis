{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab8974-33cb-42c3-bb2e-b2b325480c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pyannote.database import registry, FileFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081b7b9-2328-4490-889c-787436300c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "registry.load_database(\"/work/proy/AMI-diarization-setup/pyannote/database.yml\")\n",
    "dataset = registry.get_protocol(\"AMI-SDM.SpeakerDiarization.mini\", {\"audio\": FileFinder()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667dc4d-c5f9-49df-af41-d44d67aa056d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "#Function to convert to WAV if necessary\n",
    "def ensure_wav(audio_path):\n",
    "    # Convert to a Path object (if it's not already one)\n",
    "    audio_path = Path(audio_path)\n",
    "    \n",
    "    if audio_path.suffix != '.wav':  # Check if the file extension is not '.wav'\n",
    "        # Define the output path\n",
    "        wav_path = audio_path.with_suffix('.wav')\n",
    "        # Convert to WAV using ffmpeg\n",
    "        subprocess.call(['ffmpeg', '-i', str(audio_path), str(wav_path), '-y'])\n",
    "        return wav_path\n",
    "    return audio_path\n",
    "\n",
    "# Iterate through the dataset and ensure all files are WAV\n",
    "for file in dataset.test():\n",
    "    audio_path = file['audio']\n",
    "    wav_audio_path = ensure_wav(audio_path)\n",
    "    print(f'Processed file path: {wav_audio_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e4c204-93b4-41d9-8e89-225467ce4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_speakers = 4 #@param {type:\"integer\"}\n",
    "\n",
    "language = 'English' #@param ['any', 'English']\n",
    "\n",
    "model_size = 'large' #@param ['tiny', 'base', 'small', 'medium', 'large']\n",
    "\n",
    "\n",
    "model_name = model_size\n",
    "if language == 'English' and model_size != 'large':\n",
    "  model_name += '.en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c92ea-820b-4c8a-a1ca-c4da1d415fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/openai/whisper.git > /dev/null\n",
    "!pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44f7ee-c884-4d58-b9f6-24d661c13620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import datetime\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "import pyannote.audio\n",
    "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "embedding_model = PretrainedSpeakerEmbedding(\n",
    "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    device=torch.device(\"cuda:2\"))\n",
    "\n",
    "from pyannote.audio import Audio\n",
    "from pyannote.core import Segment\n",
    "\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pyannote.core import Segment, Annotation  # ADDITION\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate  # ADDITION\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score  # ADDITION\n",
    "from sklearn.metrics import confusion_matrix  # ADDITION\n",
    "from scipy.optimize import linear_sum_assignment  # ADDITION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bcc5a7-49a8-4374-abd4-476eb557ebdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\")\n",
    "model = whisper.load_model(model_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae358d86-1acb-4ca4-a113-88bea83e43cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_duration(wav_audio_path):\n",
    "    with contextlib.closing(wave.open(str(wav_audio_path), 'r')) as f:\n",
    "        frames = f.getnframes()\n",
    "        rate = f.getframerate()\n",
    "        duration = frames / float(rate)\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a3bff-66eb-4ce1-ac09-a028d0d4398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(wav_audio_path):\n",
    "    result = model.transcribe(str(wav_audio_path))\n",
    "    return result[\"segments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a00de3-6403-45ba-987d-3e28b2c8373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = Audio()\n",
    "def segment_embedding(segment, wav_audio_path, duration):\n",
    "    start = segment[\"start\"]\n",
    "    end = min(duration, segment[\"end\"])  # Ensure the end time is within file bounds\n",
    "    clip = Segment(start, end)\n",
    "    waveform, sample_rate = audio.crop(wav_audio_path, clip)\n",
    "    embedding = embedding_model(waveform[None])  # Extract embedding\n",
    "    return embedding.squeeze()  # Remove any extra dimensions, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd880c26-b45d-4f07-bc84-6b70f291dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(embeddings, num_speakers=4):\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=num_speakers,\n",
    "        metric='euclidean',  # MODIFICATION: Use 'cosine' affinity\n",
    "        linkage='average'\n",
    "    ).fit(embeddings)\n",
    "    return clustering.labels_\n",
    "\n",
    "# from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# def perform_clustering(embeddings, num_speakers=4):\n",
    "#     clustering = SpectralClustering(\n",
    "#         n_clusters=4,  # Number of speakers\n",
    "#         affinity='nearest_neighbors',\n",
    "#         assign_labels='kmeans',\n",
    "#         random_state=42\n",
    "#         ).fit(embeddings)\n",
    "#     return clustering.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fdedb-e5e0-4352-82a9-4adade07f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(embeddings, labels, num_speakers, title=\"Speaker Diarization Clusters (PCA Visualization)\"):\n",
    "    # Perform PCA to reduce the dimensionality of embeddings to 2D\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Create a color palette for speakers\n",
    "    unique_labels = np.unique(labels)\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_labels))\n",
    "\n",
    "    # Loop over each label and plot the embeddings\n",
    "    for idx, label in enumerate(unique_labels):\n",
    "        # Select points belonging to the current label\n",
    "        indices = np.where(labels == label)\n",
    "        label_embeddings = embeddings_2d[indices]\n",
    "\n",
    "        # Plot points for the current label\n",
    "        plt.scatter(\n",
    "            label_embeddings[:, 0],\n",
    "            label_embeddings[:, 1],\n",
    "            label=f'Label {label}',\n",
    "            color=colors(idx % 10),\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "    # Add title, labels, and legend\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.legend(loc='best', title=\"Labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164442a-787a-4beb-b30f-e68bdd16fc5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main processing loop for each file in the test set\n",
    "for file in dataset.test():\n",
    "    # Ensure WAV file for the current audio\n",
    "    wav_audio_path = ensure_wav(file['audio'])\n",
    "    print(f\"Processing file: {wav_audio_path}\")\n",
    "    \n",
    "    # Get the audio duration\n",
    "    duration = get_audio_duration(wav_audio_path)\n",
    "    \n",
    "    # Transcribe the audio file using Whisper\n",
    "    segments = transcribe_audio(wav_audio_path)\n",
    "\n",
    "     # Initialize the embeddings array and lists for labels\n",
    "    embeddings = np.zeros(shape=(len(segments), 192))  # Assuming 192 is the embedding size\n",
    "    true_labels = []  # ADDITION: To store ground truth labels for each segment\n",
    "\n",
    "    # Get ground truth annotation\n",
    "    ground_truth = file['annotation']\n",
    "    \n",
    "    # Process each segment and extract embeddings and true labels\n",
    "    for i, segment in enumerate(segments):\n",
    "        embedding = segment_embedding(segment, wav_audio_path, duration)\n",
    "        embeddings[i] = embedding\n",
    "\n",
    "        # Create a Segment object for the current segment\n",
    "        seg = Segment(segment['start'], segment['end'])\n",
    "\n",
    "        # Get the speaker(s) from ground truth that overlap with the segment\n",
    "        speakers = ground_truth.crop(seg).labels()\n",
    "\n",
    "        # Handle cases where there might be multiple or no speakers\n",
    "        if speakers:\n",
    "            true_labels.append(speakers[0])  # Take the first speaker\n",
    "        else:\n",
    "            true_labels.append('Unknown')\n",
    "\n",
    "    # Replace NaN values with 0 in embeddings (useful for handling missing data)\n",
    "    embeddings = np.nan_to_num(embeddings)\n",
    "    \n",
    "    # Map ground truth labels to integers\n",
    "    unique_speakers = list(set(true_labels))\n",
    "    speaker_to_int = {speaker: idx for idx, speaker in enumerate(unique_speakers)}\n",
    "    true_labels_int = np.array([speaker_to_int[speaker] for speaker in true_labels])\n",
    "\n",
    "    # Determine the number of unique speakers\n",
    "    num_speakers = len(unique_speakers)\n",
    "    print(f\"Number of speakers in ground truth: {num_speakers}\")\n",
    "    \n",
    "    # Perform clustering to assign speaker labels\n",
    "    labels = perform_clustering(embeddings, num_speakers=num_speakers)  # MODIFICATION\n",
    "    \n",
    "    # Compute clustering metrics before mapping\n",
    "    ari = adjusted_rand_score(true_labels_int, labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels_int, labels)\n",
    "\n",
    "    print(f\"Adjusted Rand Index (ARI) before mapping: {ari:.4f}\")\n",
    "    print(f\"Normalized Mutual Information (NMI) before mapping: {nmi:.4f}\")\n",
    "    \n",
    "    # Mapping algorithm (Hungarian algorithm)\n",
    "    confusion = confusion_matrix(true_labels_int, labels)\n",
    "\n",
    "    # Apply the Hungarian algorithm\n",
    "    row_ind, col_ind = linear_sum_assignment(-confusion)\n",
    "\n",
    "    # Create a mapping from predicted labels to true labels\n",
    "    label_mapping = {col_ind[i]: row_ind[i] for i in range(len(col_ind))}\n",
    "\n",
    "    # Map the predicted labels to the ground truth labels\n",
    "    mapped_labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "    # Compute clustering metrics after mapping\n",
    "    ari_mapped = adjusted_rand_score(true_labels_int, mapped_labels)\n",
    "    nmi_mapped = normalized_mutual_info_score(true_labels_int, mapped_labels)\n",
    "\n",
    "    print(f\"Adjusted Rand Index (ARI) after mapping: {ari_mapped:.4f}\")\n",
    "    print(f\"Normalized Mutual Information (NMI) after mapping: {nmi_mapped:.4f}\")\n",
    "    \n",
    "    # Plot PCA visualization with predicted labels\n",
    "    plot_pca(embeddings, labels, num_speakers=num_speakers, title=f\"PCA Clustering for {wav_audio_path.stem} (Predicted Labels)\")\n",
    "\n",
    "    # Plot PCA visualization with mapped labels\n",
    "    plot_pca(embeddings, mapped_labels, num_speakers=num_speakers, title=f\"PCA Clustering for {wav_audio_path.stem} (Mapped Labels)\")\n",
    "\n",
    "    # Plot PCA visualization with ground truth labels\n",
    "    plot_pca(embeddings, true_labels_int, num_speakers=num_speakers, title=f\"PCA Clustering for {wav_audio_path.stem} (Ground Truth Labels)\")\n",
    "\n",
    "    # Assign the speaker labels to each segment using mapped labels\n",
    "    for i in range(len(segments)):\n",
    "        segments[i][\"speaker\"] = f'SPEAKER {mapped_labels[i] + 1}'  # MODIFICATION\n",
    "\n",
    "    # Save the transcript with speaker labels and timestamps\n",
    "    transcript_file_path = f\"{wav_audio_path.stem}_transcript.txt\"\n",
    "    with open(transcript_file_path, \"w\") as f:\n",
    "        for i, segment in enumerate(segments):\n",
    "            if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
    "                f.write(f\"\\n{segment['speaker']} {str(datetime.timedelta(seconds=round(segment['start'])))}\\n\")\n",
    "            f.write(f\"{segment['text']} \")\n",
    "    \n",
    "    print(f\"Saved transcript and embeddings for {wav_audio_path.stem}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f62524-0a80-4584-8cb2-8a1b6d9f4373",
   "metadata": {},
   "source": [
    "#Calculating DER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f1402-9953-4f97-aaef-07a105d3cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in dataset.test():\n",
    "    print(file)\n",
    "    print(file.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c5289e-fa2b-4459-aeac-42e28a7672f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in dataset.test():\n",
    "    ground_truth = file['annotation']  # This is the ground truth annotation\n",
    "    print(f\"Ground truth annotation for file {file['uri']}:\")\n",
    "    print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4749f4f-37d7-4b94-8640-9b8e2caf6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_segments(segments, labels):\n",
    "#     merged_segments = []\n",
    "#     merged_labels = []\n",
    "\n",
    "#     start, end, current_label = segments[0]['start'], segments[0]['end'], labels[0]\n",
    "#     for i in range(1, len(segments)):\n",
    "#         if labels[i] == current_label:\n",
    "#             end = segments[i]['end']\n",
    "#         else:\n",
    "#             merged_segments.append({'start': start, 'end': end})\n",
    "#             merged_labels.append(current_label)\n",
    "#             start, end, current_label = segments[i]['start'], segments[i]['end'], labels[i]\n",
    "\n",
    "#     merged_segments.append({'start': start, 'end': end})\n",
    "#     merged_labels.append(current_label)\n",
    "\n",
    "#     return merged_segments, merged_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51320148-d96b-4339-acdd-c5abc8dceb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_predictions_to_annotation(merged_segments, merged_labels):\n",
    "#     \"\"\"\n",
    "#     Convert merged segments and labels to pyannote.core.annotation.Annotation.\n",
    "#     Args:\n",
    "#         merged_segments (list): List of merged segment dictionaries with 'start' and 'end' keys.\n",
    "#         merged_labels (list): Cluster labels for each merged segment.\n",
    "#     Returns:\n",
    "#         Annotation: Predicted annotation.\n",
    "#     \"\"\"\n",
    "#     annotation = Annotation()\n",
    "#     for segment, label in zip(merged_segments, merged_labels):\n",
    "#         annotation[Segment(segment['start'], segment['end'])] = f\"SPEAKER {label + 1}\"\n",
    "#     return annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d991a-81e5-4c32-812e-b8581c265b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in dataset.test():\n",
    "    ground_truth = file['annotation']  # Ground truth annotation\n",
    "\n",
    "    # Create predicted annotation using mapped labels\n",
    "    predicted_annotation = Annotation()\n",
    "\n",
    "    for segment, label in zip(segments, mapped_labels):\n",
    "        predicted_annotation[Segment(segment['start'], segment['end'])] = f\"SPEAKER {label + 1}\"\n",
    "\n",
    "    # Evaluate DER\n",
    "    metric = DiarizationErrorRate()\n",
    "    der = metric(ground_truth, predicted_annotation)\n",
    "    print(f\"DER for {file['uri']}: {der:.2%}\")\n",
    "\n",
    "    # Detailed DER breakdown\n",
    "    detailed_der = metric(ground_truth, predicted_annotation, detailed=True)\n",
    "    print(f\"False Alarm: {detailed_der['false alarm']:.2%}\")\n",
    "    print(f\"Missed Detection: {detailed_der['missed detection']:.2%}\")\n",
    "    print(f\"Confusion: {detailed_der['confusion']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3451dcef-8e3e-474f-b4ac-2be9e97c2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import notebook\n",
    "\n",
    "notebook.crop = Segment(0,60)  # Focus on a smaller time window for analysis\n",
    "notebook.plot_annotation(ground_truth, legend=True)\n",
    "notebook.plot_annotation(predicted_annotation, legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fdfc3b-1dfb-4068-ac45-4ed94e9f5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ground truth extent: {ground_truth.get_timeline().extent()}\")\n",
    "print(f\"Prediction extent: {predicted_annotation.get_timeline().extent()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc62361-d52b-4907-9c08-e8268cf324a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the intersection between ground truth and predicted timelines\n",
    "overlap = ground_truth.get_timeline().crop(predicted_annotation.get_timeline(), mode='intersection')\n",
    "\n",
    "# Calculate overlap coverage\n",
    "overlap_duration = overlap.duration()\n",
    "prediction_duration = predicted_annotation.get_timeline().duration()\n",
    "\n",
    "coverage = overlap_duration / prediction_duration if prediction_duration > 0 else 0\n",
    "print(f\"Overlap coverage: {coverage:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d4f4e-52e9-49e5-818d-59ffdaf77d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "thesis_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
