{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1a433-cdec-4f9d-83b5-110de1feccfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!PYANNOTE_DATABASE_CONFIG=\"/work/proy/AMI-diarization-setup/pyannote/database.yml\" pyannote-database info AMI-SDM.SpeakerDiarization.mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33938639-831c-4e9d-a6b2-f0fb41670380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyannote.database import registry, FileFinder\n",
    "\n",
    "registry.load_database(\"/work/proy/AMI-diarization-setup/pyannote/database.yml\")\n",
    "dataset = registry.get_protocol(\"AMI-SDM.SpeakerDiarization.mini\", {\"audio\": FileFinder()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138116e1-68f8-4b32-b461-fedeaf4f56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f2943",
   "metadata": {},
   "source": [
    "GPU availability check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3929b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240159f1",
   "metadata": {},
   "source": [
    "Dedicating GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41eedb-ede4-4746-b964-1f4c711f2da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fdfe59",
   "metadata": {},
   "source": [
    "Overlapped Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9505a121-1871-46c8-aab2-8525846c2e01",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Create OSD task & fine-tune \"pyannote/segmentation-3.0\" model\n",
    "# from pyannote.audio.tasks import OverlappedSpeechDetection\n",
    "# from pyannote.audio.core.model import Model\n",
    "# import pytorch_lightning as pl\n",
    "\n",
    "# osd_task = OverlappedSpeechDetection(\n",
    "#     dataset,      # same AMI mini dataset\n",
    "#     duration=4.0, # 2s chunks\n",
    "#     batch_size=16\n",
    "# )\n",
    "\n",
    "# osd_model = Model.from_pretrained(\"pyannote/segmentation-3.0\", use_auth_token=True)\n",
    "# osd_model.to(device)\n",
    "# osd_model.task = osd_task\n",
    "\n",
    "# osd_trainer = pl.Trainer(\n",
    "#     max_epochs=60,   # quick example (increase for better performance)\n",
    "#     accelerator=\"gpu\",\n",
    "#     devices=1\n",
    "# )\n",
    "# osd_trainer.fit(osd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4ae11-1b48-4061-b0f0-a9dc17e3db8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyannote.audio.pipelines import OverlappedSpeechDetection as OSDPipeline\n",
    "# osd_pipeline = OSDPipeline(osd_model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb729a1-362f-4b66-9f39-ed67e436d4a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyannote.pipeline import Optimizer\n",
    "# validation_files = list(dataset.development())\n",
    "# optimizer = Optimizer(osd_pipeline)\n",
    "\n",
    "# best = optimizer.tune(validation_files, n_iterations=150, show_progress=True)\n",
    "# print(\"Best parameters:\", best)\n",
    "\n",
    "# # Re-instantiate pipeline with best parameters\n",
    "# best_params = optimizer.best_params\n",
    "# osd_pipeline = OSDPipeline(osd_model).instantiate(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac417c-2e54-46c8-b224-25fcdf8526bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyannote.core import Annotation, Timeline\n",
    "\n",
    "# def timeline_to_annotation(timeline: Timeline, label=\"OVERLAP\") -> Annotation:\n",
    "#     annotation = Annotation()\n",
    "#     for segment in timeline:\n",
    "#         annotation[segment] = label\n",
    "#     return annotation\n",
    "\n",
    "# from pyannote.metrics.detection import DetectionErrorRate\n",
    "\n",
    "# osd_metric = DetectionErrorRate()\n",
    "\n",
    "# for file in dataset.test():\n",
    "#     # Hypothesis: OverlappedSpeechDetection pipeline returns an *Annotation* already\n",
    "#     overlap_hyp_annot = osd_pipeline(file)\n",
    "#     print(\"Hypothesis labels:\", overlap_hyp_annot.labels())\n",
    "\n",
    "\n",
    "#     # Reference: get_overlap() returns a *Timeline* that you must convert\n",
    "#     overlap_ref_tl = file[\"annotation\"].get_overlap()\n",
    "#     overlap_ref_annot = timeline_to_annotation(overlap_ref_tl, label=\"OVERLAP\")\n",
    "#     print(\"Reference labels:\", overlap_ref_annot.labels())\n",
    "\n",
    "#     # Pass them both as Annotations\n",
    "#     osd_metric(\n",
    "#         reference=overlap_ref_annot,\n",
    "#         hypothesis=overlap_hyp_annot,\n",
    "#         uem=file.get(\"annotated\", None),\n",
    "#         uri=file[\"uri\"]\n",
    "#     )\n",
    "\n",
    "# print(osd_metric.report(display=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea15ed",
   "metadata": {},
   "source": [
    "Pretrained Pipeline and its components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c46867-fe70-45c9-b9fc-51b1eb823597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "pretrained_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=True) \n",
    "pretrained_pipeline.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45359a0d-cd42-41a9-92de-ed3a22c56a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"segmentation_model: {pretrained_pipeline.segmentation_model}\")\n",
    "print(f\"embedding: {pretrained_pipeline.embedding}\")\n",
    "print(f\"klustering: {pretrained_pipeline.klustering}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160cb5ab-98fb-4337-bc49-6ecd18340d0c",
   "metadata": {},
   "source": [
    "Baseline (Default Pipeline run along with latencies and no.of segments detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f532f1c-2fc1-46c7-ad38-56f606615b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from pyannote.core import Annotation\n",
    "\n",
    "# Initializing the Diarization Error Rate metric\n",
    "metric = DiarizationErrorRate()\n",
    "\n",
    "# Dictionaries to store latency and segment counts per file\n",
    "latency_dict = {}\n",
    "segment_count_dict = {}\n",
    "\n",
    "# Record the overall start time\n",
    "overall_start_time = time.time()\n",
    "\n",
    "for file in dataset.test():\n",
    "    # Start timer for the individual file\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Applying the pretrained pipeline.\n",
    "    diarization_result: Annotation = pretrained_pipeline(file[\"audio\"])\n",
    "    \n",
    "    # Ensuring synchronization so timing is accurate, since we are using GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # End timer for the individual file\n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "    latency_dict[file['uri']] = latency\n",
    "    \n",
    "    # number of segments produced\n",
    "    number_of_segments = sum(1 for _ in diarization_result.itertracks())\n",
    "    segment_count_dict[file['uri']] = number_of_segments\n",
    "\n",
    "    # DER with detailed metrics.\n",
    "    detailed_der = metric(\n",
    "        file[\"annotation\"],\n",
    "        diarization_result,\n",
    "        uem=file[\"annotated\"],\n",
    "        detailed=True\n",
    "    )\n",
    "    \n",
    "    print(f\"File: {file['uri']} - DER: {detailed_der['diarization error rate']:.2f}\")\n",
    "    print(f\"  False Alarm: {detailed_der['false alarm']:.2f}\")\n",
    "    print(f\"  Miss: {detailed_der['missed detection']:.2f}\")\n",
    "    print(f\"  Confusion: {detailed_der['confusion']:.2f}\")\n",
    "    print(f\"  Processed in {latency:.2f} seconds, producing {number_of_segments} segments.\\n\")\n",
    "\n",
    "# Record the overall end time\n",
    "overall_end_time = time.time()\n",
    "\n",
    "\n",
    "if latency_dict:\n",
    "    mean_latency = sum(latency_dict.values()) / len(latency_dict)\n",
    "else:\n",
    "    mean_latency = 0\n",
    "\n",
    "print(f\"Mean processing time for all files: {mean_latency:.2f} seconds.\")\n",
    "\n",
    "#optional\n",
    "# print(\"Latency per file:\", latency_dict)\n",
    "# print(\"Segment count per file:\", segment_count_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46344c67",
   "metadata": {},
   "source": [
    "Visualizations for Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09740108-1ed6-4d56-ab25-20ea7daa6b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "file[\"annotation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9423a521-ff1d-4c27-bab3-989072386efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ef920-6dff-4107-b126-80f3b3e632ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def overlay_annotations(ground_truth, hypothesis, file_id=\"\"):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    \n",
    "    # Plot ground truth segments in blue \n",
    "    for segment, _, label in ground_truth.itertracks(yield_label=True):\n",
    "        ax.hlines(\n",
    "            y=1.2, \n",
    "            xmin=segment.start, \n",
    "            xmax=segment.end, \n",
    "            colors='b', \n",
    "            linewidth=10, \n",
    "            label='Ground Truth'\n",
    "        )\n",
    "    \n",
    "    # Plot hypothesis segments in red \n",
    "    for segment, _, label in hypothesis.itertracks(yield_label=True):\n",
    "        ax.hlines(\n",
    "            y=0.8, \n",
    "            xmin=segment.start, \n",
    "            xmax=segment.end, \n",
    "            colors='r', \n",
    "            linewidth=10, \n",
    "            label='Prediction'\n",
    "        )\n",
    "    \n",
    "    ax.set_yticks([0.8, 1.2])\n",
    "    ax.set_yticklabels(['Prediction', 'Ground Truth'])\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    \n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for file in dataset.test():\n",
    "    if file[\"uri\"] == \"ES2004a\":\n",
    "        \n",
    "        diarization_result = pretrained_pipeline(file[\"audio\"])\n",
    "        overlay_annotations(file[\"annotation\"], diarization_result, file_id=file[\"uri\"])\n",
    "        break  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa71aa1-66e9-4fe5-9c5d-ade1df8a55e5",
   "metadata": {},
   "source": [
    "#DER+Total_no._segments+Total_latency_for_eachfile+overall_latency for NOTSOFAR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cfd604-52a8-4faa-9e20-3df152e8ff7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# import torch\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # Load the dataset in streaming mode.\n",
    "# dataset = load_dataset(\"microsoft/NOTSOFAR\", streaming=True)\n",
    "\n",
    "# # Define the number of files to process.\n",
    "# num_files_to_process = 30\n",
    "\n",
    "# total_audio_duration = 0.0  # in seconds\n",
    "# file_counter = 0\n",
    "# latency_list = []  # to store per-file latencies\n",
    "\n",
    "# overall_start_time = time.time()\n",
    "\n",
    "# for file in dataset[\"test\"]:\n",
    "#     if file_counter >= num_files_to_process:\n",
    "#         break\n",
    "\n",
    "#     # Convert audio array to a torch tensor and cast to float.\n",
    "#     waveform = torch.tensor(file[\"audio\"][\"array\"]).float()\n",
    "\n",
    "#     # Adjust dimensions: if mono, add a channel dimension.\n",
    "#     if waveform.ndim == 1:\n",
    "#         waveform = waveform.unsqueeze(0)\n",
    "#     elif waveform.shape[0] < waveform.shape[1]:\n",
    "#         waveform = waveform.transpose(0, 1)\n",
    "\n",
    "#     sample_rate = file[\"audio\"][\"sampling_rate\"]\n",
    "#     # Calculate the duration of this audio file in seconds.\n",
    "#     duration = waveform.shape[1] / sample_rate\n",
    "#     total_audio_duration += duration\n",
    "\n",
    "#     # Prepare the input dictionary for the pipeline.\n",
    "#     audio_input = {\n",
    "#         \"waveform\": waveform,\n",
    "#         \"sample_rate\": sample_rate\n",
    "#     }\n",
    "\n",
    "#     # Measure processing time for this file.\n",
    "#     file_start = time.time()\n",
    "#     _ = pretrained_pipeline(audio_input)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.synchronize()\n",
    "#     file_end = time.time()\n",
    "\n",
    "#     file_latency = file_end - file_start\n",
    "#     latency_list.append(file_latency)\n",
    "    \n",
    "#     #print(f\"Processed file {file_counter+1} in {file_latency:.2f} seconds (duration: {duration:.2f} seconds).\")\n",
    "    \n",
    "#     file_counter += 1\n",
    "\n",
    "# overall_end_time = time.time()\n",
    "# total_processing_time = overall_end_time - overall_start_time\n",
    "\n",
    "# average_latency_per_file = sum(latency_list) / len(latency_list)\n",
    "# latency_per_audio_second = total_processing_time / total_audio_duration\n",
    "\n",
    "# print(f\"\\nTotal audio duration processed: {total_audio_duration:.2f} seconds\")\n",
    "# print(f\"Total processing time for {num_files_to_process} files: {total_processing_time:.2f} seconds\")\n",
    "# print(f\"Average latency per file: {average_latency_per_file:.2f} seconds\")\n",
    "# print(f\"Processing latency per second of audio: {latency_per_audio_second:.4f} seconds/second\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fbd37e-7f2d-4578-b33f-9807b46b27ff",
   "metadata": {},
   "source": [
    "Segmentwise_latency(Initial_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc12c5-1b96-421e-8209-cb94c4181ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# from pyannote.core import Annotation\n",
    "\n",
    "# segment_latencies = {}\n",
    "\n",
    "# for file in dataset.test():\n",
    "    \n",
    "#     file[\"pretrained pipeline\"] = pretrained_pipeline(file)\n",
    "#     diarization_result: Annotation = file[\"pretrained pipeline\"]\n",
    "\n",
    "#     # Storing latency information for the current file\n",
    "#     segment_latencies[file[\"uri\"]] = []\n",
    "\n",
    "#     for segment in diarization_result.get_timeline():\n",
    "#         # Get start time for latency measurement\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         _ = pretrained_pipeline({\"audio\": file[\"audio\"], \"segment\": segment})\n",
    "\n",
    "#         # Get end time and calculate latency\n",
    "#         end_time = time.time()\n",
    "#         segment_latency = end_time - start_time\n",
    "\n",
    "#         # Save segment and its latency\n",
    "#         segment_latencies[file[\"uri\"]].append((segment, segment_latency))\n",
    "\n",
    "#     # Print segment-wise latency for the file\n",
    "#     print(f\"File: {file['uri']}\")\n",
    "#     for seg, latency in segment_latencies[file[\"uri\"]]:\n",
    "#         print(f\"  Segment: [{seg.start:.2f} -> {seg.end:.2f}] - Latency: {latency:.2f} seconds\")\n",
    "\n",
    "# # To access latencies later\n",
    "# # print(segment_latencies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b28fb8-1786-4132-84ff-c75c3ca061da",
   "metadata": {},
   "source": [
    "Finetuning the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a4591-4d04-40fc-82c1-aacce4eeac6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyannote.audio import Model\n",
    "model_train = Model.from_pretrained(\"pyannote/segmentation\", use_auth_token=True)\n",
    "model_train.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804483bb-3ac8-4389-b527-d3d02bfe8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model_train.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a17c51-e7ee-40ef-a7ce-61a44ba2326d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyannote.audio.tasks import Segmentation\n",
    "task = Segmentation(\n",
    "    dataset, \n",
    "    duration=model_train.specifications.duration, \n",
    "    max_num_speakers=len(model_train.specifications.classes), \n",
    "    batch_size=32,\n",
    "    num_workers=2, \n",
    "    loss=\"bce\", \n",
    "    vad_loss=\"bce\")\n",
    "model_train.task = task\n",
    "model_train.prepare_data()\n",
    "model_train.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e60ac-cb61-4edd-b173-7d9d74d157f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    RichProgressBar,\n",
    ")\n",
    "\n",
    "# we use Adam optimizer with 1e-4 learning rate\n",
    "def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "model_train.configure_optimizers = MethodType(configure_optimizers, model_train)\n",
    "\n",
    "\n",
    "# we monitor DER on the validation set\n",
    "# and use to keep the best checkpoint and stop early\n",
    "monitor, direction = task.val_monitor\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    save_top_k=1,\n",
    "    every_n_epochs=1,\n",
    "    save_last=False,\n",
    "    save_weights_only=False,\n",
    "    filename=\"{epoch}\",\n",
    "    verbose=False,\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    min_delta=0.0,\n",
    "    patience=10,\n",
    "    strict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "callbacks = [RichProgressBar(), checkpoint, early_stopping]\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(accelerator=\"gpu\", \n",
    "                  devices = [1],\n",
    "                  callbacks=callbacks, \n",
    "                  max_epochs=20,\n",
    "                  strategy = \"dp\",\n",
    "                  gradient_clip_val=0.5)\n",
    "\n",
    "\n",
    "trainer.fit(model_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2625b4a-5970-43d0-8efa-e876d1eceafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = checkpoint.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996d6ab-4226-499a-b1ed-d12d7be4b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_hyperparameters = pretrained_pipeline.parameters(instantiated=True)\n",
    "pretrained_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bd6653-d1e1-4b03-934e-65062a2452d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finetuned_model = Model.from_pretrained(checkpoint.best_model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3bdb4-bb11-42de-8b87-621a76d3c5fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finetuned_model.to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c237a-2367-4290-8102-11b42b2c54a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in finetuned_model.named_parameters():\n",
    "    print(f\"{name} is on device {param.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e95c6e",
   "metadata": {},
   "source": [
    "Best Segmentation Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b18ee-b0b3-40d1-9745-87f3603ccbe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from pyannote.pipeline import Optimizer\n",
    "\n",
    "\n",
    "pipeline = SpeakerDiarization(\n",
    "    segmentation=finetuned_model,\n",
    "    clustering=\"OracleClustering\",  \n",
    ")\n",
    "\n",
    "# min_duration_off to zero for optimization\n",
    "pipeline.freeze({\"segmentation\": {\"min_duration_off\": 0.0}})\n",
    "\n",
    "# Initializing optimizer\n",
    "optimizer = Optimizer(pipeline)\n",
    "dev_set = list(dataset.development())  # Full development set\n",
    "\n",
    "# iteration limit and best_loss threshold for early stopping\n",
    "max_iterations = 30\n",
    "best_loss = 1.0\n",
    "loss_threshold = 0.1\n",
    "\n",
    "# Tuning segmentation threshold\n",
    "iterations = optimizer.tune_iter(dev_set, show_progress=True)\n",
    "for i, iteration in enumerate(iterations):\n",
    "    current_loss = iteration['loss']\n",
    "    print(f\"Iteration {i + 1}, Segmentation threshold: {iteration['params']['segmentation']['threshold']}, Loss: {current_loss}\")\n",
    "\n",
    "    # Update best loss and check for early stopping\n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "    if best_loss < loss_threshold or i >= max_iterations - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57257ef4-4a12-4eb3-9819-93d8b36ccdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_segmentation_threshold = optimizer.best_params[\"segmentation\"][\"threshold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5989c-ba1c-42fa-a0e9-3ce175578d9a",
   "metadata": {},
   "source": [
    "Best Clustering Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1ffe5-95c7-49f4-892c-a6af6d378b70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = SpeakerDiarization(\n",
    "    segmentation=finetuned_model,\n",
    "    embedding=pretrained_pipeline.embedding,\n",
    "    embedding_exclude_overlap= False, #pretrained_pipeline.embedding_exclude_overlap, \n",
    "    clustering=pretrained_pipeline.klustering,\n",
    ").to(device)\n",
    "\n",
    "pipeline.freeze({\n",
    "    \"segmentation\": {\n",
    "        \"threshold\": best_segmentation_threshold,\n",
    "        \"min_duration_off\": 0.0,\n",
    "    },\n",
    "    \"clustering\": {\n",
    "        \"method\": \"centroid\",\n",
    "        \"min_cluster_size\": 15,\n",
    "    },\n",
    "})\n",
    "\n",
    "optimizer = Optimizer(pipeline)\n",
    "iterations = optimizer.tune_iter(dev_set, show_progress=False)\n",
    "best_loss = 1.0\n",
    "for i, iteration in enumerate(iterations):\n",
    "    print(f\"Best clustering threshold so far: {iteration['params']['clustering']['threshold']}\")\n",
    "    if i > 70: break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119bcd2-d21b-4b9d-9fa5-eee00761d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clustering_threshold = optimizer.best_params[\"clustering\"][\"threshold\"]\n",
    "print(f\"Best Clustering Threshold: {best_clustering_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd984678-4812-43d4-8542-48f786a001df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#final DER on the test set\n",
    "import time\n",
    "finetuned_pipeline = SpeakerDiarization(\n",
    "    segmentation=finetuned_model,\n",
    "    embedding=pretrained_pipeline.embedding,\n",
    "    embedding_exclude_overlap= False, #pretrained_pipeline.embedding_exclude_overlap,\n",
    "    clustering=pretrained_pipeline.klustering,\n",
    ").to(device)\n",
    "\n",
    "finetuned_pipeline.instantiate({\n",
    "    \"segmentation\": {\n",
    "        \"threshold\": best_segmentation_threshold,\n",
    "        \"min_duration_off\": 0.0,\n",
    "    },\n",
    "    \"clustering\": {\n",
    "        \"method\": \"centroid\",\n",
    "        \"min_cluster_size\": 15,\n",
    "        \"threshold\": best_clustering_threshold,\n",
    "    },\n",
    "})\n",
    "\n",
    "metric = DiarizationErrorRate()\n",
    "latencies = []  \n",
    "segment_count_dict = {}\n",
    "\n",
    "\n",
    "for file in dataset.test():\n",
    "    start_time = time.time()\n",
    "    file[\"finetuned pipeline\"] = finetuned_pipeline(file)\n",
    "\n",
    "    latency = time.time() - start_time\n",
    "    latencies.append(latency)\n",
    "\n",
    "    file_der=metric(file[\"annotation\"], file[\"finetuned pipeline\"], uem=file[\"annotated\"])\n",
    "    \n",
    "    print(f\"File: {file['uri']} - DER: {file_der:.2f} - Latency: {latency:.2f} s\")\n",
    "    \n",
    "\n",
    "average_latency = sum(latencies) / len(latencies) if latencies else 0.0\n",
    "print(f\"Average pipeline latency per file: {average_latency:.2f} s\")\n",
    "\n",
    "print(f\"The finetuned pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric):.1f}% on {dataset.name} test set.\")\n",
    "final_report_df = metric.report(display=False)\n",
    "print(final_report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77d373-97eb-4924-b01d-1c3f09b3a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in dataset.test():\n",
    "    # Process the file with the pipeline\n",
    "    file[\"finetuned pipeline\"] = finetuned_pipeline(file)\n",
    "    \n",
    "    # Retrieve the timeline of detected segments\n",
    "    timeline = file[\"finetuned pipeline\"].get_timeline()\n",
    "    \n",
    "    # Count the number of segments in the timeline\n",
    "    num_segments = len(timeline)\n",
    "    \n",
    "    print(f\"File: {file['uri']} - Segments detected: {num_segments}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d4d9f",
   "metadata": {},
   "source": [
    "Visualizations for Finetuned Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd80ea9-b2f0-4897-88c3-26363d1cb645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def overlaying_annotations(ground_truth, hypothesis, file_id=\"\"):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    \n",
    "    # Plot ground truth segments in blue \n",
    "    for idx, (segment, _, label) in enumerate(ground_truth.itertracks(yield_label=True)):\n",
    "        if idx == 0:\n",
    "            ax.hlines(\n",
    "                y=1.2, \n",
    "                xmin=segment.start, \n",
    "                xmax=segment.end, \n",
    "                colors='b', \n",
    "                linewidth=10, \n",
    "                label='Ground Truth'\n",
    "            )\n",
    "        else:\n",
    "            ax.hlines(\n",
    "                y=1.2, \n",
    "                xmin=segment.start, \n",
    "                xmax=segment.end, \n",
    "                colors='b', \n",
    "                linewidth=10\n",
    "            )\n",
    "    \n",
    "    # Plot predicted segments from the finetuned pipeline in red\n",
    "    for idx, (segment, _, label) in enumerate(hypothesis.itertracks(yield_label=True)):\n",
    "        if idx == 0:\n",
    "            ax.hlines(\n",
    "                y=0.8, \n",
    "                xmin=segment.start, \n",
    "                xmax=segment.end, \n",
    "                colors='r', \n",
    "                linewidth=10, \n",
    "                label='Prediction'\n",
    "            )\n",
    "        else:\n",
    "            ax.hlines(\n",
    "                y=0.8, \n",
    "                xmin=segment.start, \n",
    "                xmax=segment.end, \n",
    "                colors='r', \n",
    "                linewidth=10\n",
    "            )\n",
    "    \n",
    "    \n",
    "    ax.set_yticks([0.8, 1.2])\n",
    "    ax.set_yticklabels(['Prediction', 'Ground Truth'])\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    \n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with the finetuned pipeline on a specific test file (e.g., file with uri \"TS3003a\")\n",
    "for file in dataset.test():\n",
    "    if file[\"uri\"] == \"ES2004a\":\n",
    "        \n",
    "        file[\"finetuned pipeline\"] = finetuned_pipeline(file)\n",
    "        overlaying_annotations(file[\"annotation\"], file[\"finetuned pipeline\"], file_id=file[\"uri\"])\n",
    "        break  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc935777",
   "metadata": {},
   "source": [
    "Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063999e5-c6e1-499f-abab-cd5540e887f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "def run_pipeline(file):\n",
    "    \n",
    "    file[\"finetuned pipeline\"] = finetuned_pipeline(file)\n",
    "    file_der = metric(file[\"annotation\"], file[\"finetuned pipeline\"], uem=file[\"annotated\"])\n",
    "    return file_der\n",
    "\n",
    "#profiler instance\n",
    "profiler = cProfile.Profile()\n",
    "\n",
    "profiler.enable()\n",
    "# Process all files in the test set\n",
    "for file in dataset.test():\n",
    "    run_pipeline(file)\n",
    "profiler.disable()\n",
    "\n",
    "s = io.StringIO()\n",
    "\n",
    "ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n",
    "ps.print_stats()\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65f17a-6809-4e6a-af80-d08ba045b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "import os\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "for file in dataset.test():\n",
    "    file[\"finetuned pipeline\"] = finetuned_pipeline(file)\n",
    "    _ = metric(file[\"annotation\"], file[\"finetuned pipeline\"], uem=file[\"annotated\"])\n",
    "\n",
    "profiler.disable()\n",
    "profiler.dump_stats(\"profile.prof\")\n",
    "print(\"Profile stats dumped to profile.prof\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
